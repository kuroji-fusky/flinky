from crawl_utils import *
from constants import *

import argparse
import json
import time
import os
import sys

parser = argparse.ArgumentParser()

parser.add_argument("-db", "--use-db", action="store_true",
                    help="Enables from all data")

parser.add_argument("-D", "--delay", type=int, default=6,
                    help="Delay each request, default is 6 seconds")

parser.add_argument("-t", "--timeout", type=int, default=8,
                    help="Define the timeout of each request, default is 15 seconds")

parser.add_argument("-r", "--retries", type=int, default=8,
                    help="Retries the request once a request timeouts, default is 8 tries before the script quits prematurely")

parser.add_argument("-v", "--verbose",  action="store_true",
                    help="Enables verbose logging")


args = parser.parse_args()

GLOBAL_TIMEOUT: int = args.timeout
GLOBAL_DELAY: int = args.delay
GLOBAL_RETRIES: int = args.retries


def _delay(sleep_sec):
    print(f"Sleeping for {sleep_sec} seconds")

    time.sleep(sleep_sec)


def make_file(filename: str, contents, *, json_mode=False):
    """Creates and appends content from a file for caching

    Args:
        filename (str): Your file name must have prefixes so you don't wanna die
        contents (_type_): yes
        json_mode (bool, optional): Enable JSON dumping
    """
    with open(filename, "a+", encoding="utf-8") as f:
        if not json_mode:
            f.write(contents)
        else:
            json.dump(contents, f, ensure_ascii=True, indent=2)


# Create the cache file and add initial timestamps to keep track of any new or deleted entries
# the articles themselves are going to be cached
cached_last_time = 0

# TODO: Store all the articles here from cache, if available
heroes_articles: list[dict[str, str]] = []


class __req_url(RequestWrapper):
    def __init__(self, url: str, referer=None):
        super().__init__(url, referer, GLOBAL_TIMEOUT, GLOBAL_RETRIES)


class ScraperConfig:
    allowlist: list[str]
    ignore_filters: list[tuple[str, str]]


CURRENT_TIME = round(time.time())


def pre_init():
    # TODO: wrap this from a class
    try:
        with open("fandom_cache.txt", "r", encoding="utf-8") as f:
            print(f.readlines())

    except FileNotFoundError:
        print("Creating cache file...")

        make_file("fandom_cache.txt", f"lastcheck {CURRENT_TIME}\n")

    # Creates a special directory for images and metadata, adds a .gitignore that ignores the entire contents
    if not os.path.exists("data/"):
        os.makedirs("data/")
        make_file("data/.gitignore", f"# generated by scraper.py\n*")

    # (2) Read the config file
    with open("scraper-config.yml", "r") as f:
        import yaml
        _scraper_config = yaml.load(f, Loader=yaml.Loader)

    ignore_filters_parsed = [list(dict.items(a))[0]
                             for a in _scraper_config["ignore-filters"]]

    scraper_config = ScraperConfig()
    scraper_config.allowlist = _scraper_config["allowlist"]
    scraper_config.ignore_filters = ignore_filters_parsed

    print(f"{len(scraper_config.allowlist)} items found in the allowlist")


def main():
    pre_init()

    _delay(GLOBAL_DELAY)

    # (4) Scrape logic
    initial_req_url = f"https://{HEROES_BASE_URL}"

    # TODO: use `httpx` instead of `requests` lol
    h_page_initial = __req_url(f"{initial_req_url}/wiki/Category:Animals").soup  # noqa

    h_article_el = h_page_initial.select(".category-page__members-for-char a.category-page__member-link")  # noqa

    # h_article_links = soup_utils.extract_links(h_article_el, prefix=initial_req_url)  # noqa
    # h_article_text = soup_utils.extract_text_content(h_article_el)

    # for link, text in zip(h_article_links, h_article_text):
    #     heroes_articles.append({
    #         "link": link,
    #         "text": text,
    #     })

    # Set the "Last" button URL, if its a match, get the remaining items, then start scraping the articles found
    end_link_marker = soup_utils.extract_links(
        h_page_initial.select_one(
            ".category-page__pagination a:last-child"
        )
    )[0]

    next_incoming_url = soup_utils.extract_links(
        h_page_initial.select_one(
            ".category-page__pagination a:nth-child(2)"
        )
    )[0]

    # TODO: add a checkpoint file for a worse case scenario when a program exits unexpectedly

    # Recursively find every entry until it hits `end_link_marker`, which will blank out `next_incoming_url`, effectively ending the loop
    while next_incoming_url is not None and isinstance(next_incoming_url, str) and next_incoming_url.strip():
        _delay(GLOBAL_DELAY)

        # End the loop by setting it to None
        if next_incoming_url == end_link_marker:
            next_incoming_url = None


if __name__ == "__main__":
    main()
